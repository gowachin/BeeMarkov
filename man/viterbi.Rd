% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/BeeMarkov.R
\name{viterbi}
\alias{viterbi}
\title{viterbi}
\usage{
viterbi(file, pos_training = "raw_data/mus_cpg_app.fa",
  neg_training = "raw_data/mus_tem_app.fa", l_word_pos = 1,
  l_word_neg = 1, n_train = 1160, n_ana = 1, l_c = 1000,
  l_nc = 125000)
}
\arguments{
\item{file}{a file (fasta) to read and to run viterbi on}

\item{pos_training}{a file (fasta) to read and train the positive model}

\item{neg_training}{a file (fasta) to read and train the negative model}

\item{l_word_pos}{a value for lengths of words for the model. Equal to the "order of the model + 1"}

\item{l_word_neg}{a value fof lengths of words for the model. Equal to the "order of the model + 1"}

\item{n_train}{number of sequences to train with}

\item{n_ana}{number of sequences to analyse}

\item{l_c}{mean length of a CpG+ region}

\item{l_nc}{mean length of a CpG- region}
}
\description{
Compute a data.frame explaining loglikelyhood of every base of a sequence
with a Viterbi algorithme based of a model of transition between 2 different models. These models are 
trained with 2 differents datasets.
}
\author{
Jaunatre Maxime <maxime.jaunatre@etu.univ-grenoble-alpes.fr>
}
